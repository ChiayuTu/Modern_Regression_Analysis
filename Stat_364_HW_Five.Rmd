---
title: "Stat_364_HW_Five"
author: "Chiayu Tu (Louis Tu)"
date: "2022-11-14"
output: 
  pdf_document:
    latex_engine : xelatex
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(ggplot2)
library(faraway)
library(tidyverse)
library(ellipse)
```

## Question One
Researchers at National Institutes of Standards and Technology (NIST) collected pipeline data on ultrasonic measurements of the depth of defects in the Alaska pipeline in the field. The depth of the defects were then remeasured in the laboratory. These measurements were performed in six different batches. It turns out that this batch effect is not significant and so can be ignored in the analysis that follows. The laboratory measurements are more accurate than the in-field measurements, but more time consuming and expensive. We want to develop a regression equation for correcting the in-field measurements.

### a
Fit a regression model Lab ~ Field. Check for non-constant variance.

```{r 1a}
model_p <- lm(Lab ~Field, data = pipeline)
summary(model_p)

plot(pipeline$Field, pipeline$Lab, col = as.factor(pipeline$Batch), cex=1.5)
abline(coef(model_p),lty=5)
plot(residuals(model_p) ~ Field, pipeline, main ="Residuals versus log(time) for simple linear model")
```


### b
We wish to use weights to account for the non-constant variance. Here we split the range of Field into 12 groups of size nine (except for the last group which has only eight values). Within each group, we compute the variance of Lab as varlab and the mean of Field as meanfield. Supposing pipeline is the name of your data frame, the following R code will make the needed computations:

```{r 1b}
require(nlme)
i <- order(pipeline$Field)
npipe <- pipeline[i,]
ff <- gl(12,9)[-108]
meanfield <- unlist(lapply(split(npipe$Field, ff), mean))
varlab <- unlist(lapply(split(npipe$Lab, ff), var))

plot(log10(varlab) ,log10(meanfield))
df<-data.frame(cbind(as.numeric(meanfield),as.numeric(varlab)))
df <- df[-c(12),]
lm.var.model <- lm(log(varlab) ~ log(meanfield) , data= df)
summary(lm.var.model)

pipeline<- pipeline[with(pipeline, order(Field)), ]
a0 <-10^summary(lm.var.model)$coefficients[1]
a1 <-summary(lm.var.model)$coefficients[2]
var.lab <- a0 * pipeline$Field^a1
se.lab <- sqrt(var.lab)
plot(pipeline$Field,pipeline$Lab, pch='*', ylim = c(0,100), main = "Field versus Lab with error bars")
arrows(pipeline$Field, pipeline$Lab-se.lab,pipeline$Field, pipeline$Lab+se.lab, length=0.05, angle=90, code=3)
pipeline$lab.var <- var.lab
wlm.fit <- gls( Lab ~ Field, data=pipeline, weights = ~ var.lab)
summary(wlm.fit)

plot(pipeline$Field, pipeline$Lab)
abline(coef(wlm.fit),lty=5)
plot(residuals(wlm.fit) ~ Field,pipeline, main ="Residuals versus Field for weighted regression ")
```

### c
An alternative to weighting is transformation. Find transformations on Lab and/or Field so that in the transformed scale the relationship is approximately linear with constant variance. You may restrict your choice of transformation to square root, log and inverse.

```{r 1c}
model_p_1 <- lm((Lab)^0.5 ~log(Field), data = pipeline)
summary(model_p_1)

plot(log(pipeline$Field),pipeline$Lab^0.5, col=as.factor(pipeline$Batch))
legend("topright",title="Batch",legend= unique(pipeline$Batch), fill=1:length(pipeline$Batch))
abline(coef(model_p_1),lty=5)
plot(residuals(model_p_1) ~ Field, pipeline)
```

## Question Two
Using the ozone data, fit a model with O3 as the response and temp, humidity and ibh as predictors. Use the Box–Cox method to determine the best transformation on the response.

```{r 2}
require(MASS)
data(ozone)

model_o <- lm(O3 ~ temp + humidity + ibh, data = ozone)
boxcox(model_o, plotit=T, lambda=seq(0.1, 0.5, by=0.1))


```

## Question Three
Use the prostate data with lpsa as the response and the other variables as predictors. Implement the following variable selection methods to determine the “best” model:

### a
Backward elimination

```{r 3a}
model_l <- lm(lpsa ~ ., prostate)
summary(model_l)

model_l <- update(model_l, . ~ . - gleason)
summary(model_l)

model_l <- update(model_l, . ~ . - lcp)
summary(model_l)

model_l <- update(model_l, . ~ . - pgg45)
summary(model_l)

model_l <- update(model_l, . ~ . - age)
summary(model_l)

model_l <- update(model_l, . ~ . - lbph)
summary(model_l)

sumary(lm(lpsa ~ gleason + lcp + pgg45 + age + lbph, prostate))
```

### b
AIC

```{r 3b}
require(leaps)
model_l_1 <- regsubsets(lpsa ~., prostate)
rs <- summary(model_l_1)
rs$which

AIC <- 50 * log(rs$rss / 50) + (2:8) * 2
plot(AIC ~ I(1:8), ylabs = "AIC", xlab = "Number of Predictors")
```

### c
Adjusted R^2

```{r 3c}
plot(1:8, rs$adjr2, xlab = "No. of parameters", ylab = "Adjusted R-squared")
which.max(rs$adjr2)
```

### d
Mallows Cp

```{r 3d}
plot(2:9, rs$cp, xlab="No. of Parameters", ylab="Cp Statistic")
abline(0,1)
```

## Question Four
Use the seatpos data with hipcenter as the response.

### a 
Fit a model with all eight predictors. Comment on the effect of leg length on the response.

```{r 4a}
model_s <- lm(hipcenter ~ ., data=seatpos)
summary(model_s)
```

### b
Compute a 95% prediction interval for the mean value of the predictors.

```{r 4b}
predict(model_s, interval = "confidence")
```

### c
Use AIC to select a model. Now interpret the effect of leg length and compute the prediction interval. Compare the conclusions from the two models.

```{r 4c}
model_s_1 <- regsubsets(hipcenter ~ ., data=seatpos)
rsc <- summary(model_s_1)
rsc$which

AIC_hip <- nrow(seatpos)*log(rsc$rss/nrow(seatpos)) + (2:9)*2
AIC_hip

model_s_2 <- lm(hipcenter ~ Age + Ht, data=seatpos)
summary(model_s_2)
```


